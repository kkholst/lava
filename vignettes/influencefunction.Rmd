---
title: Working with Influence Functions 
author: Klaus KÃ¤hler Holst
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    toc_depth: 2
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
vignette: >
  %\VignetteIndexEntry{Working with Influence Functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
---

```{r  include=FALSE }
library("lava")
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\newcommand{\indep}{\!\perp\!\!\!\!\perp\!} 
\newcommand{\pr}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathbb{V}\!\text{ar}}
\newcommand{\cov}{\mathbb{C}\!\ov{text}}
\newcommand{\IC}{\operatorname{IC}}
\newcommand{\one}{\mathbf{1}}
<!-- \newcommand{\Dto}{\rightsquigarrow} -->
\newcommand{\Dto}{\overset{\mathcal{D}}{\longrightarrow}}
\newcommand{\bm}[1]{\mathbf{#1}}

# Influence functions


Estimators that have parametric convergence rates can often be fully
characterized by their *influence function* (IF), also referred to as an influence
curve or canonical gradient. The IF allows for the direct estimation of properties of the estimator, including its asymptotic variance. Moreover,
estimates of the IF enable the simple combination and transformation of
estimators into new ones. 
This vignette describes how to estimate and manipulate IFs using the R-package `lava`.


Formally, let \(Z_1,\ldots,Z_n\) be iid, \(Z_i=(Y_{i},A_{i},W_{i})\sim P_0\), and \(\widehat{\theta}\) a
consistent estimator for the parameter \(\theta\in\mathbb{R}^p\). When
\(\widehat{\theta}\) is a *regular and asymptotic linear* (RAL) estimator, it has
a unique iid decomposition \begin{align*} \sqrt{n}(\widehat{\theta}-\theta) =
\frac{1}{\sqrt{n}}\sum_{i=1}^n \IC(Z_i; \theta) + o_p(1), \end{align*} where the
function \(\IC\) is the unique *Influence Function* s.t. \(\mathbb{E}\{\IC(Z_{i};
\theta)\}=0\) and \(\var\{\IC(Z_{i}; \theta)^{2}\}<\infty\) [@tsiatis2006semiparametric;
@vaart_1998_asymp]. The influence
function thus fully characterizes the estimator, and by the central limit
theorem it follows that the estimator converges weakly to a Gaussian distribution
\[
\sqrt{n}(\widehat{\theta}-\theta) \Dto
\mathcal{N}(0, \var\{\IC(Z; \theta)\}),
\] where the empirical variance of the plugin estimator, \(\pr_{n}\IC(Z;
\widehat{\theta})^{\otimes 2} = \frac{1}{n}\sum_{i=1}^n \IC(Z_{i};
\widehat{\theta})\IC(Z_{i};
\widehat{\theta})^{\top}\) can be used to obtain a consistent estimate of the
asymptotic variance.

The IFs are easily derived for the parameters of many parametric statistical
models as illustrated in the [next example sections](#example-generalized-linear-model). More generally, the IF can
also be derived for a smooth target parameter \(\Psi:
\mathcal{P}\to\mathbb{R}\) where \(\mathcal{P}\) is a family of probability
distributions forming the statistical model, which often can be left completely
non-parametric. 
Formally, the parameter must be *pathwise differentiability* see [@vaart_1998_asymp]
in the sense that there exists linear bounded function  \(\dot\Psi\colon
L_{2}(P_{0})\to\mathbb{R}\) such that 
\(
[\Psi(P_{t}) - \Psi(P_{0}))]t^{-1} \to \dot\Psi(P_{0})(g) 
\)
as \(t\to 0\) for any parametric submodel \(P_t\) 
with score model \(g(z)= \partial/(\partial t) \log (p_t)(z)|_{t=0}\).  Riesz's
representation theorem then tells us that the directional derivative has a 
unique representer, \(\phi_{P_{0}}\) lying in the closure of the submodel score
space (the *tangent space*), s.t.
\begin{align*}
    \dot\Psi(P_0)(g) = \langle\phi_{P_0}, g\rangle =
    \int \phi_{P_0}(Z)g(X)\,dP_0
\end{align*}
The unique representer is exactly the IF which can be found by solving the above
integral equation.
For more details on how to derive influence
functions, we refer to [@targetedlearning_2011; @hines2022].

As an example we might be interested in the target parameter \(\Psi(P) =
\E_P(Z)\) which can be shown to have the unique (and thereby efficient)
influence function \(Z\mapsto Z-\E_P(Z)\) under the non-parametric model. 
Another target parameter could be \(\Psi_{a}(P) = \E_{P}[\E_{P}(Y\mid A=a, W)]\) which is often
a key interest in causal inference and which has the IF 
\begin{align*}
\IC(Y,A,W; P) = \frac{\one(A=a)}{\pr(A=a\mid W)}(Y-\E_{P}[Y\mid A=a,W]) + \E_{P}[Y\mid
A=a,W] - \Psi_{a}(P)
    \end{align*}
See section on [average treatment effects](#average-treatment-effects).


# Examples
    
To illustrate the methods we consider data arising from the model \(Y_{ij} \sim
Bernoulli\{\operatorname{expit}(X_{ij} + A_{i} + W_{i})\}, A_{i} \sim
Bernoulli\{\operatorname{expit}(W_{i})\}\) 
with independent covariates
\(X_{ij}\sim\mathcal{N}(0,1), U_{i}\sim\mathcal{N}(0,1)\). 
```{r sim_model}
m <- lvm() |>
    regression(y1 ~ x1 + a + w) |>
    regression(y2 ~ x2 + a + w) |>
  regression(y3 ~ x3 + a + w) |>
  regression(a ~ w) |>
  distribution(~ y1 + y2 + y3 + a, value = binomial.lvm()) |>
  distribution(~id, value = Sequence.lvm(integer = TRUE))
```

We simulate from the model where \(Y_3\) is only observed for half of the subjects
```{r simulate}
n <- 5e2
dw <- sim(m, n, seed = 1) |>
  transform(y3 = y3 * ifelse(id > n / 2, NA, 1))
lava:::Print(dw)
dl <- mets::fast.reshape(dw, varying="y")
lava:::Print(dl)
```

## Example: population mean

The main functions for working with influence functions are 

- `estimate` which prepares a model object and estimates the IF and
  corresponding robust standard errors. Can also be used to transform model
  parameters by application of the Delta Theorem.
- `merge` method for combining estimates via their estimated IFs
- `IC` method to extract the estimated IF

Here we first consider the problem of estimating the IF of the mean. For the
simulated data let
\(\widehat{p}_1\) denote the MLE of the proportion
\(\pr(Y_{1}=1)\), it then follows that
\[
\sqrt{n}\{\widehat{p}_1 - \pr(Y_{1}=1)\} = \frac{1}{\sqrt{n}}\sum_{i=1}^{n}
\one(Y_{1i}=1) - \pr(Y_{1}=1)
\] 
and hence the IF is given by \(\one(Y_{1i}=1) - \pr(Y_{1}=1)\). 

To estimate this parameter and its IF we will use the `estimate` function

```{r}
inp <- as.matrix(dw[, c("y1", "y2", "y3")])
e <- estimate(inp[, 1, drop = FALSE])
e
```
The reported standard errors from the `estimate` method
are the robust standard errors obtained from the IF.
    The variance estimate and the parameters can be extracted with the `vcov`
    and `coef`methods.
The IF itself can be extracted with the `IC` method:
```{r}
IC(e) |> Print()
```

It's also possible to simultaneously estimate the proportions of each of the three
binary outcomes 
```{r}
estimate(inp)
```
or alternatively the input can be a model object, here a `mlm` object:
```{r}
e <- lm(cbind(y1, y2, y3) ~ 1, data = dw) |>
  estimate()
IC(e) |> head()
```

The IF for both the empirical mean and variance can also be estimated directly with the `IC` method:
```{r}
with(dw, IC(y1)) |> head()
```
with the point estimates being stored in the attributes "mean" and "variance".

## Example: generalized linear model

For a \(Z\)-estimator defined by the score equation \(E[U(\theta; Z)] = 0\), the
IF is given by \begin{align*} IC(Z; \theta) =
\E\Big\{\frac{\partial}{\partial\theta^\top}U(\theta; Z)\Big\}^{-1}U(\theta; Z)
\end{align*}
In particular, for a MLE the score, \(U\), is given by the partial derivative of
the log-likelihood function.

As an example, we can obtain the estimates with robust standard errors for a
logistic regression model:
```{r glm}
g1 <- glm(y1 ~ a + x1, data = dw, family = binomial)
estimate(g1)
```
We can compare that to the usual (non-robust) standard errors:
```{r glm.std}
estimate(g1, robust = FALSE)
```

The IF can be extracted from the `estimate` object or directly from the 
model object
```{r}
IC(g1) |> head()
```


The same estimates can be obtained with a 
*cumulative link regression* model which also generalizes to 
ordinal outcomes
```{r ordreg}
ordreg(y1 ~ a + x1, dw, family=binomial(logit)) |> estimate()
```

## Example: right-censored outcomes

To illustrate the methods on survival data we will use the Mayo Clinic Primary
Biliary Cholangitis Data [@therneau00surv]

```{r mets}
library("survival")
data(pbc, package="survival")
```

The Cox proportional hazards model can be fitted with the `mets::phreg` method
which can estimate the IF for both the partial likelihood parameters and the
baseline hazard. Here we fit a survival model with right-censored event times
```{r phreg}
fit.phreg <- mets::phreg(Surv(time, status > 0) ~ age + sex, data = pbc)
fit.phreg
IC(fit.phreg) |> head()
```

The IF for the baseline cumulative hazard at a specific time point can be estimated in similar way:
```{r phreg-baseline}
baseline <- function(object, time, ...) {
  ic <- IC(object, baseline = TRUE, time = time, ...)
  est <- mets::predictCumhaz(object$cumhaz, new.time = time)[1, 2]
  estimate(NULL, coef = est, IC = ic, labels = paste0("chaz:", time))
}
tt <- 2000
baseline(fit.phreg, tt)
```

The `estimate` and `IF` methods are also available for parametric survival
models via `survival::survreg`, here a Weibull model:
```{r survreg}
survival::survreg(Surv(time, status > 0) ~ age + sex, data = pbc, dist="weibull") |>
  estimate()
```

<!-- glm poisson -->
<!-- ```{r lifetable} -->
<!-- pbc2 <- mets::lifetable(Surv(time, status > 0) ~ sex, data = pbc, breaks = seq(0, 5000, length.out = 10)) -->
<!-- head(pbc2) -->
<!-- glm(events ~ offset(log(atrisk)) + sex + factor(int.end), data = pbc2, family=poisson(log)) -->
<!-- ``` -->

# Combining influence functions

A key benefit of working with the IFs of estimators is that this allows
for transforming or combining different estimates while easily deriving
the resulting IF and thereby asymptotic distribution of the new estimator. 

Let \(\widehat{\theta}_{1}, \ldots, \widehat{\theta}_{M}\) be \(M\)
different statistics with decompositions
\begin{align*}
\sqrt{n}(\widehat{\theta}_{m}-\theta_{m}) = \frac{1}{\sqrt{n}}\sum_{i=1}^{n}
\IC_k(Z_i; P) + o_{p}(1)
\end{align*}
based on i.i.d. data \(Z_1,\ldots,Z_n\). It then follows immediately that the
joint distribution of \(\widehat{\theta} - {\theta}=
(\widehat{\theta},\ldots,\widehat{\theta})^\top-
({\theta},\ldots,{\theta})^\top
\) is given by
\begin{align*}
\sqrt{n}(\widehat{\theta}-\theta) &= \frac{1}{\sqrt{n}}\sum_{i=1}^{n}
(\IC_{1}(Z_i; P),\ldots,\IC_{M}(Z_i; P))^{T} + o_{p}(1) \\
&\Dto \mathcal{N}(0,\Sigma)
\end{align*}
by CLT, and \(\frac{1}{n}\sum_{k=1}^n\epsilon_{k}^{\otimes 2} \overset{P}{\longrightarrow}\Sigma\) as \(n\to\infty\).

To illustrate this we consider two marginal logistic regression models fitted
separately for \(Y_1\) and \(Y_2\) and combine the estimates and IFs using the
`merge` method
```{r glmmarg}
g1 <- glm(y1 ~ a, family=binomial, data=dw)
g2 <- glm(y2 ~ a, family=binomial, data=dw)
e <- merge(g1, g2)
e
vcov(e)
```
As we have access to the joint asymptotic distribution we can for example test
for whether the odds-ratio is the same for the two responses: 

```{r hypo1}
estimate(e, f=cbind(0,1,0,-1), null=0)
```

## Inbalanced data

Let \(O_{1} = (Z_{1}R_{1}, R_{1}), \ldots, O_{N}=(Z_{N}R_{N}, R_{N})\) be iid with
\(R_{i}\indep Z_i\) and let the full-data IF for some estimator of a parameter
\(\theta\in\mathbb{R}^p\) be \(IC(\cdot; P)\). For convenience let
\(R_{i}=\one(i\leq n)\) then the complete-case estimator is consistent and based
on same if
\begin{align*}
\sqrt{n}(\widehat{\theta}-\theta) = \frac{1}{\sqrt{n}}\sum_{i=1}^n IC(Z_i; P) + o_p(1).
\end{align*}
This estimator can also be decomposed in terms the observed data
\(O_1,\ldots,O_n\) noting that
\begin{align*}
\sqrt{N}(\widehat{\theta}-\theta) = \frac{1}{\sqrt{N}}\sum_{i=1}^N IC(Z_i;
P)\frac{R_i N}{n} + o_p(1).
\end{align*}
where the term \(\frac{R_i N}{n}\) corresponds to an inverse probability
weighting with the empirical plugin estimate of the proportion of observed data \(R=1\).
Under a missing completely at random assumption we can therefore combine estimators that are estimated
on different datasets. Let the observed data be
\((Z_{11}R_{11}, R_{11}, Z_{21}R_{21}, R_{21}), \ldots, (Z_{1N}R_{1N}, R_{1N},
Z_{2N}R_{2N}, R_{2N}))\) with complete-case estimators \(\widehat{\theta}_1\) and
\(\widehat{\theta}_2\) for parameters \(\theta_1\) and
\(\theta_2\) based on \((Z_{11}R_{11}, \ldots, Z_{1N}R_{1N})\) and
\((Z_{21}R_{21}, \ldots, Z_{2N}R_{2N})\), respectively, and let the
corresponding IFs be
\(IC_{1}(\cdot; P)\) and \(IC_{2}(\cdot; P)\). It then follows that
\begin{align*}
\sqrt{N}\left\{
\begin{pmatrix}{}
\widehat{\theta}_1 \\
\widehat{\theta}_2 
\end{pmatrix}
- 
\begin{pmatrix}
\vphantom{\widehat{\theta}_1}\theta_1 \\
\vphantom{\widehat{\theta}_1}\theta_2 
\end{pmatrix}
\right\}
= 
\frac{1}{\sqrt{N}}\sum_{i=1}^N 
\begin{pmatrix}
IC_1(Z_{1i}; P)\frac{R_{1i}N}{R_{1\bullet}} \\
IC_2(Z_{2i}; P)\frac{R_{2i}N}{R_{2\bullet}}
\end{pmatrix} 
+ o_p(1)
\end{align*}
with \(R_{k\bullet} = \sum_{i=1}^{N}R_{ki}.\) Returning to the example, we can
combine the marginal estimates of two model objects that have been estimated
from different datasets (as the outcome \(Y_3\) is only available in half of the
data). Here we will use the overloaded `+` operator 

```{r glmmargmis}
g2 <- glm(y2 ~ 1, family = binomial, data = dw)
summary(g2)
dwc <- na.omit(dw) 
g3 <- glm(y3 ~ 1, family = binomial, data = dwc)
summary(g3)

e2 <- estimate(g2, id = dw$id)
e3 <- estimate(g3, id = "id", data=dwc)

merge(e2,e3) |> IC() |> Print()
vcov(e2 + e3)
## Same marginals as
list(vcov(e2), vcov(e3))
```

In the above example the `id` argument defines the identifier that makes it
possible to link the rows in the different IFs that should be glued together. 
If omitted then the `id` will automatically be extracted from the model-specific
`IC` method (deriving it from the original data.frame used for estimating the
model). This automatically works with all models and `IC` methods described in this document.
```{r estimatenoid}
estimate(g2) |>
  IC() |> head()
vcov(estimate(g2) + estimate(g3))
```

To force that the id variables are not overlapping between the merged model
objects, i.e., assuming that there is complete independence between the
estimates, the argument `id=NULL` can be used
```{r merge_idnull}
merge(g1, g2, id = NULL) |> (Print %++% IC)()
merge(g1, g2, id = NULL) |> vcov()
```


## Renaming and subsetting parameters

To only keep a subset of the parameters the `keep` argument can be used.  
```{r mergekeep}
merge(g1, g2, keep = c("(Intercept)", "(Intercept).1"))
```
The argument can be given either as character vector or a vector of indices:
```{r}
merge(g1,g2, keep=c(1, 3))
```
or as a vector of regular expressions
```{r}
merge(g1, g2, keep = "cept", regex = TRUE)
merge(g1, g2, keep = c("\\)$", "^a$"), regex = TRUE, ignore.case = TRUE)
```

When merging estimates unique parameter names are created. It is also possible
to rename the parameters with the `labels` argument 
```{r}
merge(g1, g2, labels = c("a", "b", "c")) |> estimate(keep = c("a", "c"))
merge(g1, g2,
      labels = c("a", "b", "c"),
      keep = c("a", "c")
)
estimate(g1, labels=c("a", "b"))
```

Finally, the `subset` argument can be used to subset the parameters and IFs
before the actual merging is being done 
```{r}
merge(g1, g2, subset="(Intercept)")
```

## TODO: Clustered data (non-iid case)

```{r}
dd <- lapply(1:4, function(x) data.frame(y = runif(2), id = 1:2))
d0 <- Reduce(rbind, dd)
e0 <- estimate(lm(y ~ 1, data = d0), id = d0[, "id"])
ee <- lapply(dd, function(x) estimate(lm(y ~ 1, data = x), id = x[, "id"]))
e <- do.call(merge, ee)

d <- data.frame(x=1:4, id=c(1,1,1,1))

e |> (rownames %++% IC) ()

IC(do.call(merge, ee))
```

```{r}

##' ## Clusters and subset (conditional marginal effects)
##' d$id <- rep(seq(nrow(d)/4),each=4)
##' estimate(g,function(p,data)
##'          list(p0=lava:::expit(p[1] + p["z"]*data[,"z"])),
##'          subset=d$z>0, id=d$id, average=TRUE)
##'
##' ## More examples with clusters:
##' m <- lvm(c(y1,y2,y3)~u+x)
##' d <- sim(m,10)
##' l1 <- glm(y1~x,data=d)
##' l2 <- glm(y2~x,data=d)
##' l3 <- glm(y3~x,data=d)
##'
##' ## Some random id-numbers
##' id1 <- c(1,1,4,1,3,1,2,3,4,5)
##' id2 <- c(1,2,3,4,5,6,7,8,1,1)
##' id3 <- seq(10)
##'
##' ## Un-stacked and stacked i.i.d. decomposition
##' IC(estimate(l1,id=id1,stack=FALSE))
##' IC(estimate(l1,id=id1))
##'
##' ## Combined i.i.d. decomposition
##' e1 <- estimate(l1,id=id1)
##' e2 <- estimate(l2,id=id2)
##' e3 <- estimate(l3,id=id3)
##' (a2 <- merge(e1,e2,e3))
##'
##' ## If all models were estimated on the same data we could use the
##' ## syntax:
##' ## Reduce(merge,estimate(list(l1,l2,l3)))
##'
##' ## Same:
##' IC(a1 <- merge(l1,l2,l3,id=list(id1,id2,id3)))
##'
##' IC(merge(l1,l2,l3,id=TRUE)) # one-to-one (same clusters)
##' IC(merge(l1,l2,l3,id=FALSE)) # independence
```


# TODO: IF building blocks: transformations and the Delta theorem

```{r}
##' ## Transformations
##' estimate(g,function(p) p[1]+p[2])
##'
##' ## Multiple parameters
##' e <- estimate(g,function(p) c(p[1]+p[2],p[1]*p[2]))
##' e
##' vcov(e)
##'
##' ## Label new parameters
##' estimate(g,function(p) list("a1"=p[1]+p[2],"b1"=p[1]*p[2]))
```

Correlation

method argument to estimate (Richardson vs complex)

Refer examples (categorical data for example)

## Linear contrasts and hypothesis testing

pairwise, -, estimate, compare, var_ic

multiple testing

pzmax,p.correct, closed testing procedure

##' ## Testing contrasts
##' estimate(g,null=0)
##' estimate(g,rbind(c(1,1,0),c(1,0,2)))
##' estimate(g,rbind(c(1,1,0),c(1,0,2)),null=c(1,2))
##' estimate(g,2:3) ## same as cbind(0,1,-1)
##' estimate(g,as.list(2:3)) ## same as rbind(c(0,1,0),c(0,0,1))
##' ## Alternative syntax
##' estimate(g,"z","z"-"x",2*"z"-3*"x")
##' estimate(g,z,z-x,2*z-3*x)
##' estimate(g,"?")  ## Wildcards
##' estimate(g,"*Int*","z")
##' estimate(g,"1","2"-"3",null=c(0,1))
##' estimate(g,2,3)

##' contr(2,n=5)
##' contr(as.list(2:4),n=5)
##' contr(list(1,2,4),n=5)
##' contr(c(2,3,4),n=5)
##' contr(list(c(1,3),c(2,4)),n=5)
##' contr(list(c(1,3),c(2,4),5))
##'
##' parsedesign(c("aa","b","c"),"?","?",diff=c(FALSE,TRUE))
##'
##' ## All pairs comparisons:
##' pdiff <- function(n) lava::contr(lapply(seq(n-1), \(x) seq(x, n)))
##' pdiff(4)


```{r}
## m <- categorical(lvm(),~x,K=5)
## regression(m,additive=TRUE) <- y~x
## d <- simulate(m,100,seed=1,'y~x'=0.1)
## l <- lm(y~-1+factor(x),data=d)
## f <- function(x) coef(lm(x~seq_along(x)))[2]
## null <- rep(mean(coef(l)), length(coef(l)))
## just need to make sure we simulate under H0: slope=0
## estimate(l,f,R=1e2,null.sim=null)
## estimate(l,f)
```

# TODO: Averaging

Let \(Z_{1},\ldots,Z_{n}\) be iid observations, \(Z_{1}\sim P\) and let  \(X_{i}\subset Z_{i}\).

Assume that \(\widehat{\theta}\) is RAL estimator of \(\theta\in\Omega\subset \mathbb{R}^{p}\)
\[\sqrt{n}(\widehat{\theta}-\theta) = \frac{1}{\sqrt{n}}\sum_{i=1}^{n}\phi(Z_{i}) + o_{p}(1).
\]
Let \(f:\mathcal{X}\times\Omega\to\mathbb{R}\) be continuous differentiable in \(\theta\)
\[\sqrt{n}\{f(X; \widehat{\theta})-f(X; \theta)\} =
\frac{1}{\sqrt{n}}\nabla_{\theta}f(X;\theta)\sum_{i=1}^{n}\phi(Z_{i}) + o_{p}(1).
\]


Let \(\Psi = Pf(X;\theta)\) and \(\widehat{\Psi} = P_{n} f(X;\widehat{\theta})\). \(P\) and \(P_{n}\) are here everywhere the integrals wrt. \(X\). It is easily verified that
\begin{align*}
\widehat{\Psi}-\Psi &= (P_{n}-P)(f(X; \theta)-\Psi) + P[f(X;\widehat{\theta})-f(X;\theta)] \\
&\quad + (P_{n}-P)[f(X;\widehat{\theta})-f(X;\theta)]
\end{align*}

From Lemma 19.24 [@vaart_1998_asymp] it follows that for the last term
\begin{align*}
\sqrt{n}(P_{n}-P)[f(X;\widehat{\theta})-f(X;\theta)] = o_{p}(1)
\end{align*}
when \(f\) for example is Lipschitz and more generally when \(f(X;\theta)\) forms a \(P\)-Donsker class.

It therefore follows that
\begin{align*}
\sqrt{n}(\widehat{\Psi}-\Psi) &= \sqrt{n}P_{n}(f(X; \theta)-\Psi) +
\frac{1}{\sqrt{n}}P\nabla_{\theta}f(X;\theta)\sum_{i=1}^{n}\phi(Z_{i}) + o_{p}(1) \\
&=
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\{f(X;\theta)-\Psi\} +
\frac{1}{\sqrt{n}}P\nabla_{\theta}f(X;\theta)\sum_{i=1}^{n}\phi(Z_{i}) + o_{p}(1)
\end{align*}
Hence the IF becomes
\[
IC(Z) = f(X;\theta)-\Psi +
[P\nabla_{\theta}f(X;\theta)]\phi(Z).
\]

estimate(average=TRUE)

##' ## Marginalize
##' f <- function(p,data)
##'   list(p0=lava:::expit(p["(Intercept)"] + p["z"]*data[,"z"]),
##'        p1=lava:::expit(p["(Intercept)"] + p["x"] + p["z"]*data[,"z"]))
##' e <- estimate(g, f, average=TRUE)
##' e
##' estimate(e,diff)
##' estimate(e,cbind(1,1))


## TODO: Average Treatment Effects

```{r}
g <- glm
```


targeted::cate
targeted::riskreg
targeted::alean

# TODO: Two-stage estimation
stack

## TODO: Measurement error
measurement.error

# TODO: Examples diagnostics and ....

diagtest, assoc, riskcomp, Ratio, Diff, odds, OR,
information_assoc, information.table, independence (not exported) information.multinomial
gkgamma, kappa


Example (McKenzie et al. 1996, Clarke et al. 1993)

50 patients from Monash Medical Centre, Melbourne
- *Diagnosis of depression* (DSM-III-R MDD, Dysthymia, Adjustment
  Disorder with Depressed Mood, Depression NOS)
- *Beck Depression Inventory (BDI)* (Beck et al., 1961)
- *General Health Questionnaire (GHQ)* (Goldberg & Williams, 1988)
 (classificaiton, see McKenzie, 1992).

| Diagnosis | BDI | GHQ |  n |
|-----------+-----+-----+----|
| \(+\)     |   1 |   1 |  7 |
| \(-\)     |   1 |   1 |  1 |
| \(-\)     |   1 |   0 |  1 |
| \(+\)     |   0 |   1 |  4 |
| \(-\)     |   0 |   1 |  2 |
| \(+\)     |   0 |   0 |  2 |
| \(-\)     |   0 |   0 | 33 |

Agreement between a diagnosis of depression,
and caseness according to the screening test?

Let \(X\in\{0,1,...,k-1\}\) and  \(Y\in\{0,1,...,l-1\}\) 
be categorical s.v. with the joint probabilities \(\bm{\pi}\) and data \(\bm{n}\)

\begin{align*}
\pi_{ij} = \pr(X=i,Y=j), \qquad
\bm{n}= 
\begin{pmatrix}
n_{11} & n_{12} & \cdots & n_{1l} \\
n_{21} & n_{22} & \cdots & n_{2l} \\
\vdots & \vdots & \ddots & \vdots \\
n_{k1} & n_{k2} & \cdots & n_{kl}
\end{pmatrix}
\end{align*}
\vspace*{-.5em}
and association measure 
# (twice continuous differentiable)
\begin{align*}
\alpha = \alpha(\bm{\pi}).
\end{align*}

We will assume that we have an asymptotically linear estimator of
\(\bm{\pi} = \bm{\pi}(\theta)\) ,
e.g., under multinomial distribution with likelihood
\begin{align*}
L(\pi_{ij}\mid n_{ij})  = A\prod_{i,j} \pi_{ij}^{n_{ij}}, \pi_{ij}\geq 0, \sum_{ij}\pi_{ij}=1
\end{align*}
\begin{align*}
\widehat{\pi}_{ij} = \frac{n_{ij}}{\sum_{i,j}n_{ij}}, \quad \widehat{\alpha} = \alpha(\widehat{\bm{\pi}})
\end{align*}


# TODO: estimate methods

coef, vcov, summary, parameter

back.transform

level

coef.summary.estimate

estimate.list
estimate.array
multinomial



# TODO: Enabling new models 

IC method
score method

# SessionInfo

```{r sessionInfo}
sessionInfo()
```

# Bibliography

